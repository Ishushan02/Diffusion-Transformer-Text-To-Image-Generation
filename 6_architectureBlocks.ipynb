{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affef73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "from CombinationFunctions import TimeEmbedding, TextEmbedding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2818350e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ccaba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedDimension = 768\n",
    "tEmbed = TimeEmbedding(embedDimension=embedDimension)\n",
    "tEmbed.to(device)\n",
    "time = torch.tensor([1000]).to(device)\n",
    "tout = tEmbed(time)\n",
    "tout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ce08f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 768]),\n",
       " torch.Size([1, 768]),\n",
       " torch.Size([1, 768]),\n",
       " torch.Size([1, 768]),\n",
       " torch.Size([1, 768]),\n",
       " torch.Size([1, 768]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AdaptiveLayerNorm(nn.Module):\n",
    "    def __init__(self, embedDimension):\n",
    "        super().__init__()\n",
    "        self.embedDimension = embedDimension\n",
    "        self.adaLN = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embedDimension, 6 * embedDimension)\n",
    "        )\n",
    "        self.scaleShiftParameters = nn.Parameter(torch.zeros(6, embedDimension))\n",
    "        nn.init.zeros_(self.adaLN[1].weight)\n",
    "        nn.init.zeros_(self.adaLN[1].bias)      \n",
    "    \n",
    "    def forward(self, t):\n",
    "        batchSize, _ = t.shape\n",
    "        t = self.adaLN(t)\n",
    "        t = t.reshape(batchSize, 6, -1)\n",
    "        gamma_msa, beta_msa, alpha_msa, gamma_mlp, beta_mlp, alpha_mlp = (\n",
    "            (self.scaleShiftParameters[None] + t).chunk(6, dim = 1)\n",
    "        )\n",
    "        gamma_msa = gamma_msa.squeeze(1)\n",
    "        beta_msa = beta_msa.squeeze(1)\n",
    "        alpha_msa = alpha_msa.squeeze(1)\n",
    "        gamma_mlp = gamma_mlp.squeeze(1)\n",
    "        beta_mlp = beta_mlp.squeeze(1)\n",
    "        alpha_mlp = alpha_mlp.squeeze(1)\n",
    "        return gamma_msa, beta_msa, alpha_msa, gamma_mlp, beta_mlp, alpha_mlp\n",
    "    \n",
    "adaNorm = AdaptiveLayerNorm(embedDimension)\n",
    "g1, b1, a1, g2, b2, a2 = adaNorm(tout)\n",
    "g1.shape, b1.shape, a1.shape, g2.shape, b2.shape, a2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac71c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shiftModulate(x, scale, shift):\n",
    "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
    "\n",
    "class ScaleShiftBlock(nn.Module):\n",
    "    def __init__(self, embedDimension):\n",
    "        super().__init__()\n",
    "        self.embedDimension = embedDimension\n",
    "        self.norm = nn.LayerNorm(embedDimension, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "    def forward(self, x, beta, gamma):\n",
    "        B, N, W = x.shape\n",
    "        x_norm = self.norm(x)\n",
    "        out = shiftModulate(x_norm, gamma, beta)\n",
    "        return out\n",
    "    \n",
    "patchify_latents = torch.randn(1, 16, 768)\n",
    "scShft = ScaleShiftBlock(embedDimension)\n",
    "out = scShft(patchify_latents, g1, b1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f385f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scaleModulate(x, scale):\n",
    "    return x * (1 + scale.unsqueeze(1))\n",
    "\n",
    "class ScaleBlock(nn.Module):\n",
    "    def __init__(self, embedDimension):\n",
    "        super().__init__()\n",
    "        self.embedDimension = embedDimension\n",
    "        self.norm = nn.LayerNorm(embedDimension, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "    def forward(self, x, alpha):\n",
    "        B, N, W = x.shape\n",
    "        x_norm = self.norm(x)\n",
    "        out = scaleModulate(x_norm, alpha)\n",
    "        return out\n",
    "    \n",
    "patchify_latents = torch.randn(1, 16, 768)\n",
    "scShft = ScaleBlock(embedDimension)\n",
    "out = scShft(patchify_latents, a1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e10f3251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embedDimension, numHeads, dropout = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        assert embedDimension%numHeads == 0, \"Embedding Dimension is Not Divisible By NumHeads\"\n",
    "        self.embedDimension = embedDimension\n",
    "        self.numHeads = numHeads\n",
    "        self.headDim = embedDimension//numHeads\n",
    "\n",
    "        self.queryKeyValue = nn.Linear(embedDimension, embedDimension * 3, bias=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.scale = self.headDim ** -0.5 \n",
    "        self.outProjection = nn.Linear(embedDimension, embedDimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        BatchSize, N, EmbedDim = x.shape\n",
    "\n",
    "        qkv = self.queryKeyValue(x)\n",
    "        qkv = qkv.reshape(BatchSize, N, 3, self.numHeads, EmbedDim // self.numHeads)\n",
    "        q, k, v = qkv.unbind(2)\n",
    "        attentionScore = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attentionScore.softmax(dim=-1)\n",
    "        out = attn @ v \n",
    "        out = out.transpose(1, 2).reshape(BatchSize, N, EmbedDim)\n",
    "        out = self.outProjection(out)\n",
    "        out = self.drop(out)\n",
    "        return out\n",
    "    \n",
    "input = torch.randn(1, 16, 768)\n",
    "msa = MultiHeadSelfAttention(embedDimension=768, numHeads=8)\n",
    "out = msa(input)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8785bd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, embedDimension, numHeads, dropout = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        assert embedDimension%numHeads == 0, \"Embedding Dimension is Not Divisible By NumHeads\"\n",
    "        self.embedDimension = embedDimension\n",
    "        self.numHeads = numHeads\n",
    "        self.headDim = embedDimension//numHeads\n",
    "\n",
    "        self.q = nn.Linear(embedDimension, embedDimension)\n",
    "        self.k = nn.Linear(embedDimension, embedDimension)\n",
    "        self.v = nn.Linear(embedDimension, embedDimension)\n",
    "        self.kv = nn.Linear(embedDimension, 2 * embedDimension)\n",
    "\n",
    "        self.outProjection = nn.Linear(embedDimension, embedDimension)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.scale = self.headDim ** -0.5 \n",
    "\n",
    "    def forward(self, x, textCondition):\n",
    "        batch, Nimg, embedDim = x.shape\n",
    "        batch, Ntext, embedDim = textCondition.shape\n",
    "\n",
    "        q = self.q(x)\n",
    "        k = self.k(textCondition)\n",
    "        v = self.v(textCondition)\n",
    "\n",
    "        q = q.view(batch, Nimg, self.numHeads, self.headDim).transpose(1, 2)\n",
    "        k = k.view(batch, Ntext, self.numHeads, self.headDim).transpose(1, 2)\n",
    "        v = v.view(batch, Ntext, self.numHeads, self.headDim).transpose(1, 2)\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn_probs = Fn.softmax(attn_scores, dim=-1)\n",
    "       \n",
    "        out = attn_probs @ v \n",
    "        out = out.transpose(1, 2).contiguous().view(batch, Nimg, embedDim)\n",
    "        out = self.outProjection(out)\n",
    "        out = self.drop(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "text = [\"Generate an Image of a Dog Eating\"]\n",
    "textModel = TextEmbedding()\n",
    "textembed = textModel(text)\n",
    "xinp = torch.randn(1, 16, 768)\n",
    "\n",
    "mca = MultiHeadCrossAttention(embedDimension, numHeads=8)\n",
    "out = mca(xinp, textembed)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eacf5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, embedDimension):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(embedDimension, embedDimension * 4)\n",
    "        self.linear2 = nn.Linear(embedDimension * 4, embedDimension)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "latents = torch.randn(1, 16, 768)\n",
    "ff = FeedForwardBlock(embedDimension)\n",
    "out = ff(latents)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "469a3608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DiTModule(nn.Module):\n",
    "    def __init__(self, embedDimension, numHeads, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.embedDimension = embedDimension\n",
    "        self.numHeads = numHeads\n",
    "\n",
    "        self.scaleShift1 = ScaleShiftBlock(embedDimension)\n",
    "        self.multiHeadselfAtten = MultiHeadSelfAttention(embedDimension, numHeads, dropout)\n",
    "        self.scale1 = ScaleBlock(embedDimension)\n",
    "\n",
    "        self.multiHeadcrossAtten = MultiHeadCrossAttention(embedDimension, numHeads, dropout)\n",
    "\n",
    "        self.scaleShift2 = ScaleShiftBlock(embedDimension)\n",
    "        self.pointwiseFeedForward = FeedForwardBlock(embedDimension)\n",
    "        self.scale2 = ScaleBlock(embedDimension)\n",
    "\n",
    "    def forward(self, imageLatents, textEmbeddings, sharedParameters):\n",
    "        gamma1, beta1, alpha1, gamma2, beta2, alpha2 = sharedParameters\n",
    "\n",
    "        x = imageLatents\n",
    "        scaleShiftOut1 = self.scaleShift1(x, gamma1, beta1)\n",
    "        selfAttnOut = self.multiHeadselfAtten(scaleShiftOut1)\n",
    "        scaleOut1 = self.scale1(selfAttnOut, alpha1)\n",
    "\n",
    "        x =  x + scaleOut1\n",
    "\n",
    "        y = self.multiHeadcrossAtten(x, textEmbeddings)\n",
    "\n",
    "        y = y + x\n",
    "\n",
    "        scaleShiftOut2 = self.scaleShift2(y, gamma2, beta2)\n",
    "        mlpOut = self.pointwiseFeedForward(scaleShiftOut2)\n",
    "        scaleOut2 = self.scale2(mlpOut, alpha2)\n",
    "\n",
    "        z = y + scaleOut2\n",
    "\n",
    "        return z\n",
    "    \n",
    "\n",
    "text = [\"Generate an Image of a Dog Eating\"]\n",
    "textModel = TextEmbedding()\n",
    "textembed = textModel(text)\n",
    "noisedlatents = torch.randn(1, 16, 768)\n",
    "adaNorm = AdaptiveLayerNorm(embedDimension)\n",
    "sharedParameters = adaNorm(tout)\n",
    "\n",
    "dit = DiTModule(embedDimension, numHeads=12)\n",
    "\n",
    "out = dit(noisedlatents, textembed, sharedParameters)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de4ea9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NDiTModule(nn.Module):\n",
    "    def __init__(self, blocks, embedDimension, numHeads, dropout = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.adaNorm = AdaptiveLayerNorm(embedDimension)\n",
    "        \n",
    "        self.eachBlocks = nn.ModuleList([\n",
    "            DiTModule(embedDimension, numHeads, dropout)\n",
    "            for _ in range(blocks)\n",
    "        ])\n",
    "        self.finalNorm = nn.LayerNorm(embedDimension, elementwise_affine=False, eps=1e-6)\n",
    "\n",
    "    def forward(self, imageLatents, textEmbeddings, timeEmbeddings):\n",
    "        \n",
    "        x = imageLatents\n",
    "        sharedParams = self.adaNorm(timeEmbeddings)\n",
    "        for block in self.eachBlocks:\n",
    "            x = block(x, textEmbeddings, sharedParams)\n",
    "\n",
    "        x = self.finalNorm(x)\n",
    "        return x\n",
    "        \n",
    "nDit = NDiTModule(blocks=12, embedDimension=embedDimension, numHeads=12, dropout=0.2)\n",
    "tEmbed = TimeEmbedding(embedDimension=embedDimension)\n",
    "tEmbed.to(device)\n",
    "time = torch.tensor([1000]).to(device)\n",
    "tout = tEmbed(time)\n",
    "\n",
    "\n",
    "text = [\"Generate an Image of a Dog Eating\"]\n",
    "textModel = TextEmbedding()\n",
    "textembed = textModel(text)\n",
    "noisedlatents = torch.randn(1, 16, 768)\n",
    "\n",
    "out = nDit(noisedlatents, textembed, tout)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b709ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a312db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
